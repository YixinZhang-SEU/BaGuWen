## 消保领域文本分析与处理系统 | 平安集团合作项目

**使用技术**： Python，LDA 模型，TF/IDF 模型，SVM监督学习算法。

**项目概述**： 根据消保领域文本特点，本项目需实现消保行业的中文文本分词、关键词提取、文本分类等功能。

**主要职责**： 

1. 使用一阶CRF模型结合ADF训练方法，获得消保领域的分词模型，并根据此和自定义词典生成文本标注和字典树， 进行文本分词； 
2. 分别使用 TF/IDF 和 LDA 主题模型计算词语碎片综合权重，再对相邻碎片关键词进行融合，生成关键短语； 
3. 分别根据关键词序列、关键词权重生成向量编码、为向量赋权，生成每一个关键词的集成决策树，以关键词为特征将文本分类。

**项目成果**： 通过公司验收，系统已嵌入到平安集团后台业务中。

> **分词**
>
> 在训练模型部分，我们需要输入若干篇已经分过词的文本作为训练集。训练基于单词的特征和(标签-过渡)特征，使用条件较为简单的**一阶条件随机场用于汉语分词和新单词检测的联合建模**。我们用**ADF算法，即基于特征频率信息的自适应在线梯度下降**对模型进行训练。ADF算法不像随机梯度下降（SGD算法）这样的所有参数都使用单一的学习率，而是**将学习率转换为与参数具有相同维度的向量**。每个参数的学习率会根据参数的频率自动调整。这个使频率更高的特征会更容易被筛选出。训练后，我们得到了一个消保领域的分词模型文件，文件中包含训练得到的词库和对应的分词权重。
>
> 在正式分词部分，我们先**预加载刚刚训练好的模型**，另外还需加载一个**自定义词典**，这个词典主要是基于公司给的关键词集合。因为这些关键词都是一些偏专有的词汇，另外还有一些例如中国银保监会这种固定机构名，我们希望分词的时候不把它们分开。后续我们会持续更新自定义词典的词。首先要构建字典树以便存储所有词。在分词时**先把文本与字典树作匹配**，摘取出所有在用户词典中已经出现的词。接下来对剩余的文本利用CRF模型生成字标注，并把有标注的词与训练得到的词库对比，在词库中就分出来，不在的话看特征概率，高于50%的话就加到词库中并分出，否则先抛弃不分词。
>
> **关键词提取**
>
> 接下来到了关键词提取的部分。首先要**对文本做一个预处理，预处理包含分词、词性标注、去停用词和文本中的杂质**几个部分。接着读取TF-IDF文件和单词话题权重文件，预计算主题突出度。然后再把词语做碎片化细分，开始清洗文本、计算词频，对每个词进行词性标注和语义角色标注，获取候选短语。对候选短语使用**TF-IDF和LDA主题模型**计算候选短语在各个主题中的权重。最后用MMR算法重排结果，减少相似候选词的冗余。
>
> TF-IDF用以评估一字词对于一个[文件集](https://baike.baidu.com/item/文件集/12724334?fromModule=lemma_inlink)或一个语料库中的其中一份文件的重要程度；LDA主题模型根据概率分布为一段文本生成主题。
>
> **文本分类**
>
> 分类器：
> 1）通过尝试不同的算法：SVM、决策树、贝叶斯方法等，择优选取，并结合集成学习综合多个弱分类器的结果，提升算法效果，算法的分类准确率从87.6%提升至96%。（1000篇文章的测试结果）
> 2）综合多个分类器结果，处理空值问题。在多个分类器都没有输出结果时，选择最有可能的权益，优先保证至少输出一种权益分类。
>
> **难点与解决**
>
> 难点：调参
>
> 参数主要是这几个，**一个是ADF学习衰减值，超参数α和β，训练轮数以及迭代次数**。数据集一个是用公司之前提供的两篇分过词的文章，另一个加上了我们自己分的三十篇文章。训练时我们调整了一些参数，对比了f-score，这个**f-score就是训练出的结果与测试集对比的准确率**，发现**学习率rate在0.20或0.25时f-score比较好**。接下来我们就用0.20作为学习衰减值，0.995和0.6分别作为超参数，训练消保领域的约XX篇文本，得到了一个约有XX词的词库。另外我们的自定义词典中有约350词。
>
> 解决：耐心调，切忌浮躁



## 分布式IP地址防碰撞分配算法寻优 | 华为合作项目

**使用技术**： C++， Python， Golang多线程，分布式算法。

项目概述： 某区域内的基站都可以为用户分配IP地址，但当该用户移动到另一基站时，新基站必须为其重新分配不重复的新IP地址，否则就会产生地址碰撞，本项目力图减少整体的碰撞概率。

**主要职责**：

1. 为解决基站节点数量发生变化时的数据迁移问题，提出使用一致性哈希算法替代华为现有算法；
2. 为解决传统一致性哈希算法的数据倾斜问题，提出将单个基站节点转化为多个虚拟节点后再映射到哈希环上；
3. 为解决单个基站节点用户数量突然增多的突发性负载问题，提出分布式染色算法显著提高每个基站节点可分配的IP地址数量。

**项目成果**： 华为使用广州市天河区基站历史数据进行测试，新算法成功将碰撞概率从11%降至1%。

> **一致性哈希**
>
> 1. 将整个哈希值空间（总共65536个ip地址）围成一个虚拟的哈希圆环
> 2. 将基站编号通过哈希函数进行哈希，确定基站在哈希环上的位置。
> 3. 使用哈希算法将ip映射到哈希环上的某一个位置，顺时针找到第一个基站即为此ip所在的基站，需要注意的是单个基站至多连接1200个ip，如果超限了就顺时针次分配到下一个基站。
>
> 一致性哈希解决了单个基站节点数量发生变化后，所有基站都需要重新分配ip的数据迁移问题。
>
> **哈希环的虚拟节点**
>
> 在基站比较少的情况下，容易因为ip分布不均匀而造成数据倾斜问题，也就是大部分ip集中在相邻的某几个基站，这种情况就称为哈希环的倾斜。为了解决这种数据倾斜问题，我们引入了虚拟节点机制，即每一个基站计算多个哈希，每个计算结果位置都为该基站的虚拟节点，一个实际物理基站节点可以对应多个虚拟节点，虚拟节点越多，哈希环上的节点就越多，ip被均匀分布的概率就越大，哈希环倾斜所带来的影响就越小，哈希算法基本不变，只是多了一步虚拟节点到实际节点的映射。
>
> **分布式染色算法**
>
> 1. 节点设置优先级
>
> 优先级按照：本节点所需色块数量，邻居数量，id大小来依次比较
>
> 2. 节点分配颜色
>
> 3. 异常处理
>
> **难点与解决**
>
> 难点：一步步想出优化的方法比较难
>
> 解决：多与小组成员讨论交流，多学习查阅现有论文，多做实验验证新想法的合理性
